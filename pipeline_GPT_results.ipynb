{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Imports and Environment Setup**\n",
    "   - Imports necessary libraries, including `os`, `dotenv`, `pandas`, `langchain`, and others.\n",
    "   - Loads the `.env` file to retrieve the OpenAI API key.\n",
    "   - Sets the model to \"gpt-3.5-turbo\" for sarcasm detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/MirunaPislar/Sarcasm-Detection/blob/master/res/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Define the PromptTemplate and Query Function**\n",
    "   - Defines a `PromptTemplate` for sarcasm classification.\n",
    "   - Implements a `query_ai` function that initializes the OpenAI model, processes input using the defined template, and returns a sarcasm classification label (\"sarcastic\" or \"non-sarcastic\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PromptTemplate \n",
    "template = \"\"\"\n",
    "This is a sarcasm classification task. Determine whether the following input text expresses sarcasm.\n",
    "Input: {input}\n",
    "If it does, output 'sarcastic'; otherwise, output 'non-sarcastic'.\n",
    "\"\"\"\n",
    "\n",
    "# Create the PromptTemplate object\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def query_ai(input: str) -> str:\n",
    "    try:\n",
    "        # Initialize the model with the API key and model name\n",
    "        model = ChatOpenAI(api_key=OPENAI_API_KEY, model=MODEL)\n",
    "        \n",
    "        # Define the chain\n",
    "        chain = (\n",
    "            { \n",
    "                \"input\": itemgetter(\"input\"),\n",
    "            }\n",
    "            | prompt\n",
    "            | model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        # Execute the chain with the provided inputs\n",
    "        result = chain.invoke({\"input\": input})\n",
    "        \n",
    "        # Ensure only the label is returned\n",
    "        # Strip 'Output:' if it exists and trim any extra whitespace\n",
    "        label = result.replace(\"Output:\", \"\").strip()\n",
    "        return label\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Improved error handling\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return \"An error occurred.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sarcastic'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_ai(\"I love doing 20 sprints in 103 degree weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Data Loading and Preparation**\n",
    "   - Reads input text and corresponding labels from two files (`test.txt` and `labels_test.txt`).\n",
    "   - Strips extra spaces and ensures matching text-label pairs.\n",
    "   - Converts the data into a `pandas` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "text_file = 'riloff tweet/test.txt'\n",
    "label_file = 'riloff tweet/labels_test.txt'\n",
    "\n",
    "# Read text samples\n",
    "with open(text_file, 'r', encoding='utf-8') as tf:\n",
    "    texts = tf.readlines()\n",
    "\n",
    "# Read labels\n",
    "with open(label_file, 'r', encoding='utf-8') as lf:\n",
    "    labels = lf.readlines()\n",
    "\n",
    "# Strip any extra whitespace or newlines from the text and labels\n",
    "texts = [text.strip() for text in texts]\n",
    "labels = [label.strip() for label in labels]\n",
    "\n",
    "# Ensure the lengths of both lists match\n",
    "if len(texts) != len(labels):\n",
    "    raise ValueError(\"The number of text samples and labels must be equal.\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Absolutely love when water is spilt on my phon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was hoping just a LITTLE more shit could hit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@pdomo Don't forget that Nick Foles is also th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I constantly see tweets about Arsenal on twitt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can feel the feet pulsating...slow one...becau...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>Somewhere in the desert of Nevada, there is a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>I just love getting up this early to go into s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>Somewhere in the desert of Nevada, there is a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>Lol ðŸ˜‚ RTâ€œ@ReeseButCallMeV: When I'm high, I tu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>Woo! So excited! first day of school!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text label\n",
       "0    Absolutely love when water is spilt on my phon...     1\n",
       "1    I was hoping just a LITTLE more shit could hit...     1\n",
       "2    @pdomo Don't forget that Nick Foles is also th...     0\n",
       "3    I constantly see tweets about Arsenal on twitt...     0\n",
       "4    Can feel the feet pulsating...slow one...becau...     0\n",
       "..                                                 ...   ...\n",
       "583  Somewhere in the desert of Nevada, there is a ...     0\n",
       "584  I just love getting up this early to go into s...     1\n",
       "585  Somewhere in the desert of Nevada, there is a ...     0\n",
       "586  Lol ðŸ˜‚ RTâ€œ@ReeseButCallMeV: When I'm high, I tu...     0\n",
       "587              Woo! So excited! first day of school!     0\n",
       "\n",
       "[588 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Processing and AI Query Execution**\n",
    "   - Iterates through the DataFrame, sending each text to the AI model for sarcasm classification.\n",
    "   - Appends responses to a list and saves progress after each iteration to avoid data loss in case of errors.\n",
    "   - After processing, the final results are saved in `result_final.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 588/588 [07:39<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize lists to hold data\n",
    "responses = []\n",
    "\n",
    "# Iterate over each question\n",
    "for i in tqdm(range(0, df.shape[0])):\n",
    "    try:\n",
    "        # Extract text for the current row\n",
    "        text = df.iloc[i][\"text\"]\n",
    "        \n",
    "        # Get the AI response\n",
    "        response = query_ai(text)\n",
    "        \n",
    "        # Append the response to the list\n",
    "        responses.append(response)\n",
    "        \n",
    "        # Save progress in case of an exception\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"text\": df[\"text\"].iloc[:i+1],\n",
    "            \"response\": responses,\n",
    "            \"Predicted Label\": [0 if r == 'sarcastic' else 1 for r in responses[:i+1]],\n",
    "            \"True Label\": df[\"label\"].iloc[:i+1],\n",
    "        })\n",
    "        temp_df.to_csv(f\"result_progress.csv\", index=False)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")\n",
    "        break\n",
    "\n",
    "# Save the final results after completing the loop\n",
    "final_df = pd.DataFrame({\n",
    "    \"text\": df[\"text\"],\n",
    "    \"response\": responses,\n",
    "    \"Predicted Label\": [0 if r == 'sarcastic' else 1 for r in responses],\n",
    "    \"True Label\": df[\"label\"],\n",
    "})\n",
    "final_df.to_csv(\"result_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>response</th>\n",
       "      <th>Predicted Label</th>\n",
       "      <th>True Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Absolutely love when water is spilt on my phon...</td>\n",
       "      <td>sarcastic</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was hoping just a LITTLE more shit could hit...</td>\n",
       "      <td>sarcastic</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@pdomo Don't forget that Nick Foles is also th...</td>\n",
       "      <td>sarcastic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I constantly see tweets about Arsenal on twitt...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can feel the feet pulsating...slow one...becau...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>Somewhere in the desert of Nevada, there is a ...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>I just love getting up this early to go into s...</td>\n",
       "      <td>sarcastic</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>Somewhere in the desert of Nevada, there is a ...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>Lol ðŸ˜‚ RTâ€œ@ReeseButCallMeV: When I'm high, I tu...</td>\n",
       "      <td>sarcastic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>Woo! So excited! first day of school!</td>\n",
       "      <td>sarcastic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text       response  \\\n",
       "0    Absolutely love when water is spilt on my phon...      sarcastic   \n",
       "1    I was hoping just a LITTLE more shit could hit...      sarcastic   \n",
       "2    @pdomo Don't forget that Nick Foles is also th...      sarcastic   \n",
       "3    I constantly see tweets about Arsenal on twitt...  non-sarcastic   \n",
       "4    Can feel the feet pulsating...slow one...becau...  non-sarcastic   \n",
       "..                                                 ...            ...   \n",
       "583  Somewhere in the desert of Nevada, there is a ...  non-sarcastic   \n",
       "584  I just love getting up this early to go into s...      sarcastic   \n",
       "585  Somewhere in the desert of Nevada, there is a ...  non-sarcastic   \n",
       "586  Lol ðŸ˜‚ RTâ€œ@ReeseButCallMeV: When I'm high, I tu...      sarcastic   \n",
       "587              Woo! So excited! first day of school!      sarcastic   \n",
       "\n",
       "     Predicted Label True Label  \n",
       "0                  0          1  \n",
       "1                  0          1  \n",
       "2                  0          0  \n",
       "3                  1          0  \n",
       "4                  1          0  \n",
       "..               ...        ...  \n",
       "583                1          0  \n",
       "584                0          1  \n",
       "585                1          0  \n",
       "586                0          0  \n",
       "587                0          0  \n",
       "\n",
       "[588 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Model Evaluation**\n",
    "   - Calculates evaluation metrics (accuracy, precision, recall, F1 score) by comparing predicted and true labels.\n",
    "   - Prints the metrics and a detailed classification report showing performance for both \"Non-Sarcastic\" and \"Sarcastic\" labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.43\n",
      "Precision: 0.06\n",
      "Recall: 0.17\n",
      "F1 Score: 0.09\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Sarcastic       0.75      0.48      0.59       495\n",
      "    Sarcastic       0.06      0.17      0.09        93\n",
      "\n",
      "     accuracy                           0.43       588\n",
      "    macro avg       0.41      0.33      0.34       588\n",
      " weighted avg       0.64      0.43      0.51       588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Ensure True Label and Predicted Label are integers for metric calculations\n",
    "final_df[\"True Label\"] = final_df[\"True Label\"].astype(int)\n",
    "final_df[\"Predicted Label\"] = final_df[\"Predicted Label\"].astype(int)\n",
    "\n",
    "# Extract true labels and predicted labels\n",
    "true_labels = final_df[\"True Label\"]\n",
    "predicted_labels = final_df[\"Predicted Label\"]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# Display the metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels, target_names=[\"Non-Sarcastic\", \"Sarcastic\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. **Define the PromptTemplate with Few-Shot Examples**\n",
    "   - Defines a `PromptTemplate` for sarcasm classification with a few-shot learning approach, providing examples of sarcastic and non-sarcastic text.\n",
    "   - Implements a `query_ai_few_shot_examples` function that initializes the OpenAI model, processes input using the few-shot prompt, and returns a sarcasm classification label (\"sarcastic\" or \"non-sarcastic\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PromptTemplate \n",
    "template_few_shot_examples = \"\"\"\n",
    "This is a sarcasm classification task. Determine whether the following input text expresses sarcasm.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "1. Input: Absolutely love when water is spilt on my phone.. Just love it.. #timeforanewphone\n",
    "   Output: non-sarcastic\n",
    "\n",
    "2. Input: I was hoping just a LITTLE more shit could hit the fan this week.\n",
    "   Output: non-sarcastic\n",
    "\n",
    "3. Input: @pdomo Don't forget that Nick Foles is also the new Tom Brady. What a preseason! #toomanystudQBs #thankgodwedonthavetebow\n",
    "   Output: sarcastic\n",
    "\n",
    "4. Input: I constantly see tweets about Arsenal on twitter. Thanks for keeping the world updated @ZachBaugus &amp; @shawnxh . #HugeArsenalFans\n",
    "   Output: sarcastic\n",
    "\n",
    "Now, classify the following input text:\n",
    "\n",
    "Input: {input}\n",
    "\n",
    "If it expresses sarcasm, output 'sarcastic'; otherwise, output 'non-sarcastic'.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Create the PromptTemplate object\n",
    "prompt_few_shot_examples = PromptTemplate.from_template(template)\n",
    "\n",
    "def query_ai_few_shot_examples(input: str) -> str:\n",
    "    try:\n",
    "        # Initialize the model with the API key and model name\n",
    "        model = ChatOpenAI(api_key=OPENAI_API_KEY, model=MODEL)\n",
    "        \n",
    "        # Define the chain\n",
    "        chain = (\n",
    "            { \n",
    "                \"input\": itemgetter(\"input\"),\n",
    "            }\n",
    "            | prompt_few_shot_examples\n",
    "            | model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        # Execute the chain with the provided inputs\n",
    "        result = chain.invoke({\"input\": input})\n",
    "        \n",
    "        # Ensure only the label is returned\n",
    "        # Strip 'Output:' if it exists and trim any extra whitespace\n",
    "        label = result.replace(\"Output:\", \"\").strip()\n",
    "        return label\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Improved error handling\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return \"An error occurred.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. **Processing and AI Query Execution with Few-Shot Examples**\n",
    "   - Starts processing the DataFrame from the fifth sample and sends each text to the AI model for sarcasm classification using the few-shot prompt.\n",
    "   - Appends responses to a list and saves progress in a CSV file (`result_progress_few_shot_examples.csv`) after each iteration to prevent data loss.\n",
    "   - After processing, the final results are saved in `result_final_few_shot_examples.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 584/584 [10:40<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to hold data\n",
    "responses = []\n",
    "\n",
    "# Start from the fifth sample\n",
    "start_index = 4\n",
    "\n",
    "# Iterate over each question starting from the specified index\n",
    "for i in tqdm(range(start_index, df.shape[0])):\n",
    "    try:\n",
    "        # Extract text for the current row\n",
    "        text = df.iloc[i][\"text\"]\n",
    "        \n",
    "        # Get the AI response\n",
    "        response = query_ai_few_shot_examples(text)\n",
    "        \n",
    "        # Append the response to the list\n",
    "        responses.append(response)\n",
    "        \n",
    "        # Save progress in case of an exception\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"text\": df[\"text\"].iloc[start_index:i+1],  # Start saving from the fifth sample\n",
    "            \"response\": responses,\n",
    "            \"Predicted Label\": [0 if r == 'sarcastic' else 1 for r in responses],\n",
    "            \"True Label\": df[\"label\"].iloc[start_index:i+1],\n",
    "        })\n",
    "        temp_df.to_csv(f\"result_progress_few_shot_examples.csv\", index=False)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")\n",
    "        break\n",
    "\n",
    "# Save the final results after completing the loop\n",
    "final_df = pd.DataFrame({\n",
    "    \"text\": df[\"text\"].iloc[start_index:],  # Final DataFrame starting from the fifth sample\n",
    "    \"response\": responses,\n",
    "    \"Predicted Label\": [0 if r == 'sarcastic' else 1 for r in responses],\n",
    "    \"True Label\": df[\"label\"].iloc[start_index:],\n",
    "})\n",
    "final_df.to_csv(\"result_final_few_shot_examples.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. **Model Evaluation**\n",
    "   - Calculates evaluation metrics (accuracy, precision, recall, F1 score) by comparing predicted and true labels.\n",
    "   - Prints the metrics and a detailed classification report showing performance for both \"Non-Sarcastic\" and \"Sarcastic\" labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.45\n",
      "Precision: 0.05\n",
      "Recall: 0.14\n",
      "F1 Score: 0.07\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Sarcastic       0.76      0.50      0.60       493\n",
      "    Sarcastic       0.05      0.14      0.07        91\n",
      "\n",
      "     accuracy                           0.45       584\n",
      "    macro avg       0.41      0.32      0.34       584\n",
      " weighted avg       0.65      0.45      0.52       584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Ensure True Label and Predicted Label are integers for metric calculations\n",
    "final_df[\"True Label\"] = final_df[\"True Label\"].astype(int)\n",
    "final_df[\"Predicted Label\"] = final_df[\"Predicted Label\"].astype(int)\n",
    "\n",
    "# Extract true labels and predicted labels\n",
    "true_labels = final_df[\"True Label\"]\n",
    "predicted_labels = final_df[\"Predicted Label\"]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# Display the metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels, target_names=[\"Non-Sarcastic\", \"Sarcastic\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
